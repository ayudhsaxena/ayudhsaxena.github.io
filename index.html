<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ayudh Saxena</title>

    <meta name="author" content="Ayudh Saxena">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ayudh Saxena
                </p>
                <p>
                  I am currently pursuing a Master's degree in <a href="https://www.ml.cmu.edu/">Machine Learning</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. Currently, I am working with <a href="https://www.cs.cmu.edu/~pradeepr/">Prof. Pradeep Ravikumar</a> on improving representation learning by using clustering as a 'context' to leverage the instrinsic structure of the data. I am also working with <a href="https://talkingtorobots.com/yonatanbisk.html">Prof. Yonatan Bisk</a> on building better social reasoning models. My primary research interests include representation learning and multimodal machine learning, with a focus on developing robust, generalizable, and semantically rich representations. I am also interested in developing intelligent systems that can do reasoning in social contexts. Recently, I have also developed an interest in the mechanistic interpretability of deep learning models, particularly using Sparse Autoencoders to uncover interpretable features. 
                </p>
                <p>
                    Previously, I worked at <a href="https://www.adobe.com/">Adobe</a> on the <a href="https://www.adobe.com/products/photoshop-express.html">Photoshop Express (PsX)</a> app, where I was a founding member of the Video and Generative AI (GenAI) teams. I contributed to introducing new video editing and generative AI capabilities into the app from scratch. During my undergraduate studies at <a href="http://www.iitkgp.ac.in/">IIT Kharagpur</a>, I developed vBeats, a framework for converting gestures into bass strokes for Indian Tabla, advised by <a href="https://sandipc-iitkgp.github.io/sandip-web/">Prof. Sandip Chakraborty</a>. This work earned the Best Poster Award at <a href="https://www.comsnets.org/archive/2024/awards.html">COMSNETS 2024</a>. Additionally, during my research internship at <a href="https://research.adobe.com/">Adobe Research</a> Bangalore, I developed Videos2Doc, an ML framework for generating documents from procedural videos, which later received a US patent.
                </p>          
                <p style="text-align:center">
                  <a href="mailto:ayudhs@cs.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/ayudh_resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=E8mlQEQAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ayudhsaxena">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/pfp.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/pfp.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

         
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id="are_image">
                    <img src="images/vbeats.png" width="160" style="opacity:1;">
                  </div>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10427116">
                  <span class="papertitle">vBeats: A Framework for Converting Gestures into
                    Bass Strokes from Indian Tabla using Smartphones</span>
                </a>
                <br>
                <a href="https://ayudhsaxena.github.io/">Ayudh Saxena</a>,
                <a href="https://sites.google.com/view/sjitiit/home">Soumyajit Chatterjee</a>,
                <a href="https://sandipc-iitkgp.github.io/sandip-web/">Sandip Chakraborty</a>,
                <br>
                <em>COMSNETS’24</em>, advised by Prof. Sandip Chakraborty
                <br>
                <a href="https://github.com/ayudhsaxena/vBeats-implementation">GitHub</a>
                /
                <a href="https://ieeexplore.ieee.org/document/10427116">IEEE Page</a>
                <p></p>
                <p>
                  vBeats is a novel approach for translating Indian Tabla gestures, captured via smartphone sensors, into corresponding bass strokes. It addresses the challenge of bridging the gap between two modalities with drastically different sampling rates. The system employs a multi-modal framework leveraging autoencoders to learn robust latent representations from inertial and audio signals, followed by LSTMs to map these representations using sequence-to-sequence learning. We also curated a dataset comprising 1.38 hours of synchronized IMU-audio data from professional Tabla players.
              </td>
            </tr>  


            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id="neurocut_image">
                    <img src="images/videos2doc.png" width="160" style="opacity:1;">
                  </div>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3490100.3516460">
                  <span class="papertitle">Videos2Doc: Generating Documents from a Collection of Procedural Videos</span>
                </a>
                <br>
                <a href="https://ayudhsaxena.github.io/">Ayudh Saxena</a>,
                <a href="https://www.linkedin.com/in/triptishukla12/?originalSubdomain=in">Tripti Shukla</a>,
                <a href="https://www.linkedin.com/in/aanisha-bhattacharyya/?originalSubdomain=in">Aanisha Bhattacharya</a>
                <a href="https://www.linkedin.com/in/jeevana-karnuthala/?originalSubdomain=in">Jeevana Kruthi Karnuthala</a>,
                <a href="https://abhinav-bohra.github.io/">Abhinav Bohra</a>
                <a href="https://www.linkedin.com/in/bhanuguda/">Bhanu Prakash Guda</a>
                <a href="https://abhilashasancheti.github.io/">Abilasha Sancheti</a>
                <a href="https://www.linkedin.com/in/niyati-chhaya-9aaa25a/?originalSubdomain=in">Niyati Chhaya</a>
                <br>
                <em>ACM IUI’22</em>, advised by Dr. Niyati Chhaya
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3490100.3516460">ACM Page</a>
                <p></p>
                <p>
                  Videos2Doc is a machine learning framework designed to automatically generate structured documents from collections of procedural videos. The primary challenge involves extracting meaningful information from the diverse multimodal content present in such videos. To accomplish this, we implemented a multi-stage pipeline. First, key video frames are selected by constructing an action graph, which is then input, along with any available textual ingredients, to an encoder-decoder-decoder model to produce coherent instructions.
                </p>
              </td>
            </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Work Experience</h2>
                    <p>
                      <b>Adobe Inc, Noida - SDE 2</b>  
                      <i>Photoshop Express Team</i> &nbsp;&nbsp;&nbsp;&nbsp;<i>Aug '22 - Aug '24</i>  
                      <ul>
                        <li>Developed a proof-of-concept for a novel GenAI-first mobile application, integrating Adobe's core Text2Image, Inpainting, and Outpainting capabilities alongside a live community feed</li>
                        <li>Designed and developed a full-fledged video editing segment within <b>Photoshop Express</b> as a founding member of the
                          Video Team, utilizing <b>SwiftUI</b> framework to create a seamless user experience.</li>
                        <li>Implemented a novel solution to generate contextually appropriate, stylistically consistent meme captions for images
                          utilizing cloud-based image-to-text models followed by GPT using ,<b>Chain of Thought</b> prompting.</li>
                        <li>Recipient of the <b>Adobe Spot</b> and <b>Adobe Star</b> award for excellent engineering work done for the year 2022-23</li>
                      </ul>
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Borrowed from Jon Barron website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
